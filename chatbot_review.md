# Campus Store Chatbot - Comprehensive Architecture Review

## Executive Summary

Your RAG chatbot has a solid foundation but contains **5 critical bugs** and several architectural issues that explain your follow-up question problems and timeout issues. Below is a prioritized list of fixes.

---

## üö® CRITICAL BUGS (Fix Immediately)

### Bug 1: Missing FAQ Retrieval Path in `retriever.py`
**Location**: `app/rag/retriever.py`, line 60+

**Problem**: Your `retrieve()` method only has code for the `instructions` branch. When `selected_collection == "faqs"`, the function returns `None` because there's no `else` clause.

**Impact**: Every FAQ query fails silently, returns `None`, and forces LLM-only responses.

**Fix**:
```python
def retrieve(self, query: str, k: int = 1, collection: str = "auto", platform: str = None):
    selected_collection = (
        self._select_collection(query)
        if collection == "auto"
        else collection
    )

    if selected_collection == "instructions":
        source_prefix = "INSTR"
        # ... existing instructions code ...
    
    else:  # ‚Üê ADD THIS BRANCH
        # FAQ retrieval path
        query_embedding = self.model.encode([query], normalize_embeddings=True)
        scores, indices = self.faq_index.search(
            np.array(query_embedding).astype("float32"), k
        )
        
        best_index = indices[0][0]
        best_score = float(scores[0][0])
        best_chunk = self.faq_chunks[best_index]
        
        match = re.search(r'Article link:\s*"?([^"\n]+)"?', best_chunk)
        article_link = match.group(1).strip() if match else None
        
        return {
            "context": best_chunk,
            "score": best_score,
            "source_id": f"FAQ_SOURCE_{best_index}",
            "article_link": article_link
        }
```

---

### Bug 2: Course Code Not Extracted in Follow-Up Flow
**Location**: `app/main.py`, lines 43-50

**Problem**: When `awaiting_course_code` is `True`, you set `course_code = message` (the entire user message) instead of extracting the course code with regex.

**Example**:
- User types: "It's BIO101 for biology"
- Current code sets: `course_code = "It's BIO101 for biology"`
- Should set: `course_code = "BIO101"`

**Fix**:
```python
if app.state.awaiting_course_code:
    course_code = extract_course_code(message)  # ‚Üê Use regex extraction
    intent = app.state.stored_intent
    platform = app.state.stored_platform
    
    # Clear state
    app.state.awaiting_course_code = False
    app.state.stored_intent = None
    app.state.stored_platform = None
```

---

### Bug 3: User Message Added to History Before State Check
**Location**: `app/main.py`, lines 37-39

**Problem**: You append the user message to `chat_history` BEFORE checking if you're in the `awaiting_course_code` state. This pollutes the conversation history with fragments.

**Example flow**:
1. Bot asks: "What's your course code?"
2. User replies: "BIO101"
3. History now contains: `[..., {role: "user", content: "BIO101"}]`
4. But "BIO101" is not a conversational message - it's a slot value

**Fix**: Move history append AFTER state handling:
```python
# ===== STATE HANDLING (LLM-FIRST FLOW) =====
if app.state.awaiting_course_code:
    # ... handle state ...
    pass
else:
    # ... detect intent ...
    pass

# ===== APPEND TO HISTORY AFTER STATE RESOLUTION =====
app.state.chat_history.append({
    "role": "user",
    "content": message
})
```

---

### Bug 4: Global Shared Chat History (Multi-User Unsafe)
**Location**: `app/main.py`, line 13

**Problem**: `app.state.chat_history` is global to the FastAPI app instance. This means:
- All users share the same conversation history
- User A's messages appear in User B's context
- Breaks completely with multiple uvicorn workers
- Breaks with concurrent requests

**Impact**: Production deployment is impossible.

**Fix**: Implement session-scoped storage (see Solution #1 below).

---

### Bug 5: Platform Filtering Re-Embeds Everything on Every Request
**Location**: `app/rag/retriever.py`, lines 49-58

**Problem**: When platform filtering is active, you:
1. Filter chunks in memory
2. Re-encode ALL filtered chunks (expensive!)
3. Build a temporary FAISS index
4. Search the temporary index

**Impact**: 
- Extremely slow (1-3 seconds per query)
- Causes timeout issues you mentioned
- Wastes CPU/memory

**Fix**: Use metadata filtering instead (see Solution #2 below).

---

## üèóÔ∏è ARCHITECTURAL ISSUES

### Issue 1: Ingest vs. Description Mismatch

**What you told me**: "We chunk documents using `\n\n` separation."

**What your code does**: `file_chunks = [text.strip()]` - you read each file as ONE chunk.

**Reality**: 
- Each FAQ file = 1 chunk
- Each instruction file = 1 chunk
- No splitting happens

**Is this bad?** Not necessarily! For instruction files, keeping the entire guide as one chunk is actually good for your use case. But you should be aware of this.

**Recommendation**: Document this clearly in your code and decide if you want true chunking.

---

### Issue 2: LLM Context Injection as Assistant Message

**Location**: `app/llm/llama_client.py`, lines 46-50

**Problem**: You inject reference context as an assistant message:
```python
messages.append({
    "role": "assistant",
    "content": f"Reference information:\n{context}"
})
```

**Why this is confusing**: The LLM sees this as something IT said previously, not as external knowledge. In multi-turn conversations, this can cause the LLM to think it already provided this information.

**Better approach**:
```python
# Option 1: Inject into system message
system_message["content"] += f"\n\nReference Information:\n{context}"

# Option 2: Inject as user message prefix
messages.append({
    "role": "user",
    "content": f"[Reference Context]\n{context}\n\n[User Question]\n{message}"
})
```

---

### Issue 3: L2 Distance on Normalized Vectors

**Location**: `retriever.py`, uses `IndexFlatL2` with `normalize_embeddings=True`

**Problem**: L2 distance on normalized vectors is mathematically awkward. Lower scores are better, but the scale is unintuitive.

**Better approach**: Use `IndexFlatIP` (inner product) which gives cosine similarity when vectors are normalized:
```python
# Replace IndexFlatL2 with IndexFlatIP
temp_index = faiss.IndexFlatIP(embeddings.shape[1])
```

**Benefit**: Higher scores = better matches (more intuitive), and you get true cosine similarity.

---

## ‚úÖ RECOMMENDED SOLUTIONS

### Solution 1: Session-Scoped Memory (Multi-User Safe)

Replace global `app.state.chat_history` with session-based storage:

**Option A: In-Memory Dictionary (Simple)**
```python
from collections import defaultdict
from datetime import datetime, timedelta

# Store sessions in memory with expiration
sessions = {}  # session_id -> {history, awaiting_course_code, stored_intent, ...}
SESSION_TIMEOUT = timedelta(hours=1)

def get_or_create_session(session_id: str):
    if session_id not in sessions:
        sessions[session_id] = {
            "history": [],
            "awaiting_course_code": False,
            "stored_intent": None,
            "stored_platform": None,
            "last_activity": datetime.now()
        }
    sessions[session_id]["last_activity"] = datetime.now()
    return sessions[session_id]

def cleanup_expired_sessions():
    now = datetime.now()
    expired = [sid for sid, data in sessions.items() 
               if now - data["last_activity"] > SESSION_TIMEOUT]
    for sid in expired:
        del sessions[sid]

@app.post("/chat", response_model=ChatResponse)
def chat(payload: ChatRequest):
    session_id = payload.session_id or "default"
    session = get_or_create_session(session_id)
    
    # Use session.history instead of app.state.chat_history
    # Use session["awaiting_course_code"] instead of app.state.awaiting_course_code
    # etc.
```

**Option B: Redis (Production-Ready)**
- Use Redis with session TTL
- Survives restarts
- Works with multiple workers
- Scales horizontally

---

### Solution 2: Pre-Filtered FAISS Indices (Fix Timeouts)

Instead of re-embedding on every request, create separate indices per platform during ingestion:

**Modified `ingest.py`**:
```python
def ingest_instructions():
    all_chunks = []
    cengage_chunks = []
    mcgraw_chunks = []
    
    for file_name in file_names:
        # ... read file ...
        chunk = f"[SOURCE_{i}] [FILE:{file_name}]\n{text.strip()}"
        all_chunks.append(chunk)
        
        if "cengage" in text.lower() or "mindtap" in text.lower():
            cengage_chunks.append(chunk)
        elif "mcgraw" in text.lower() or "connect" in text.lower():
            mcgraw_chunks.append(chunk)
    
    # Create 3 separate indices
    model = SentenceTransformer('all-MiniLM-L6-v2')
    
    # All instructions
    embeddings = model.encode(all_chunks)
    index = faiss.IndexFlatIP(embeddings.shape[1])  # ‚Üê Use IP for cosine
    index.add(embeddings)
    faiss.write_index(index, "data/instructions/faiss_index")
    
    # Cengage only
    if cengage_chunks:
        embeddings_c = model.encode(cengage_chunks)
        index_c = faiss.IndexFlatIP(embeddings_c.shape[1])
        index_c.add(embeddings_c)
        faiss.write_index(index_c, "data/instructions/faiss_index_cengage")
    
    # McGraw Hill only
    if mcgraw_chunks:
        embeddings_m = model.encode(mcgraw_chunks)
        index_m = faiss.IndexFlatIP(embeddings_m.shape[1])
        index_m.add(embeddings_m)
        faiss.write_index(index_m, "data/instructions/faiss_index_mcgraw")
```

**Modified `retriever.py`**:
```python
class FAQRetriever:
    def __init__(self):
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.faq_index = faiss.read_index(FAQ_INDEX_PATH)
        self.instructions_index = faiss.read_index(INSTRUCTIONS_INDEX_PATH)
        
        # Load platform-specific indices
        try:
            self.instructions_index_cengage = faiss.read_index(
                "data/instructions/faiss_index_cengage"
            )
        except:
            self.instructions_index_cengage = None
            
        try:
            self.instructions_index_mcgraw = faiss.read_index(
                "data/instructions/faiss_index_mcgraw"
            )
        except:
            self.instructions_index_mcgraw = None
        
        # Load chunks...
    
    def retrieve(self, query: str, k: int = 1, collection: str = "auto", platform: str = None):
        # ... selection logic ...
        
        if selected_collection == "instructions":
            # Select the right index
            if platform == "CENGAGE" and self.instructions_index_cengage:
                index = self.instructions_index_cengage
                chunks = self.cengage_chunks
            elif platform == "MCGRAW_HILL" and self.instructions_index_mcgraw:
                index = self.instructions_index_mcgraw
                chunks = self.mcgraw_chunks
            else:
                index = self.instructions_index
                chunks = self.instruction_chunks
            
            # Embed query ONCE
            query_embedding = self.model.encode([query], normalize_embeddings=True)
            
            # Search pre-built index (fast!)
            scores, indices = index.search(
                np.array(query_embedding).astype("float32"), k
            )
            
            # Return result...
```

**Performance improvement**: 2-3 seconds ‚Üí 50-200ms

---

### Solution 3: Better RAG Routing Strategy

Your current heuristic-based routing is brittle. Consider:

**Option A: Intent Classification with the LLM**
```python
def detect_intent_and_collection(message: str, llm: LlamaClient) -> dict:
    """Use LLM to classify intent and determine collection."""
    classification_prompt = (
        "Classify this user message:\n\n"
        f'"{message}"\n\n'
        "Respond with ONLY a JSON object:\n"
        '{"intent": "IA_ACCESS_ISSUE" or "GENERAL_FAQ", '
        '"collection": "instructions" or "faqs", '
        '"platform": "CENGAGE" or "MCGRAW_HILL" or null}\n\n'
        "Rules:\n"
        "- IA_ACCESS_ISSUE: User can't access digital course materials\n"
        "- instructions: User needs step-by-step help\n"
        "- faqs: User asks about policies or general info"
    )
    
    # Call LLM with classification prompt
    result = llm.chat(classification_prompt, context="", history=[])
    return json.loads(result)
```

**Option B: Dual Retrieval + Score Comparison**
```python
# Retrieve from BOTH collections
faq_result = retriever.retrieve(message, collection="faqs")
instruction_result = retriever.retrieve(message, collection="instructions", platform=platform)

# Use whichever has higher confidence
if faq_result["score"] > instruction_result["score"]:
    return faq_result
else:
    return instruction_result
```

---

### Solution 4: Fix Follow-Up Context Loss

**Root cause**: Each retrieval is independent - follow-ups don't use conversation history to disambiguate.

**Solution**: Include recent conversation context in retrieval query:

```python
def build_contextual_query(message: str, history: list) -> str:
    """Enhance query with conversation context."""
    if not history or len(history) < 2:
        return message
    
    # Get last assistant response
    last_response = None
    for msg in reversed(history):
        if msg["role"] == "assistant":
            last_response = msg["content"]
            break
    
    if not last_response:
        return message
    
    # Check if current message is a pronoun-heavy follow-up
    pronouns = ["it", "that", "this", "there", "where"]
    if any(pronoun in message.lower().split() for pronoun in pronouns):
        # Combine with previous context
        contextual_query = f"{last_response[:200]} {message}"
        return contextual_query
    
    return message

# In main.py, before retrieval:
contextual_query = build_contextual_query(message, session["history"])
retrieval = retriever.retrieve(contextual_query, ...)
```

---

## üìã IMPLEMENTATION PRIORITY

### Phase 1 (Immediate - Fixes Critical Bugs)
1. ‚úÖ Add FAQ retrieval branch to `retriever.py` (Bug #1)
2. ‚úÖ Fix course code extraction in follow-up flow (Bug #2)
3. ‚úÖ Move history append after state handling (Bug #3)

### Phase 2 (Critical - Multi-User Support)
4. ‚úÖ Implement session-scoped memory (Bug #4)
5. ‚úÖ Update CLI to generate/send session_id

### Phase 3 (Performance - Fix Timeouts)
6. ‚úÖ Pre-build platform-filtered indices (Bug #5)
7. ‚úÖ Switch to IndexFlatIP for cosine similarity

### Phase 4 (Quality - Better Responses)
8. ‚úÖ Implement contextual query enhancement for follow-ups
9. ‚úÖ Fix LLM context injection method
10. ‚úÖ Improve intent/collection routing

---

## üß™ TESTING CHECKLIST

After implementing fixes, test these scenarios:

- [ ] FAQ query: "What's the refund policy?"
- [ ] Instruction query: "How do I access McGraw Hill?"
- [ ] Follow-up: "Where can I find it?"
- [ ] Missing course code: "I can't access Cengage"
- [ ] Concurrent sessions: Open 2 CLI instances, verify histories don't mix
- [ ] Platform filtering: "McGraw Hill issue" should not return Cengage results

---

## üìä PERFORMANCE EXPECTATIONS

**Current (with bugs)**:
- FAQ queries: Fail (returns None)
- Instruction queries with platform filter: 2-3 seconds
- Follow-ups: Often timeout or wrong context

**After fixes**:
- FAQ queries: 50-100ms
- Instruction queries with platform filter: 50-200ms
- Follow-ups: 100-300ms with correct context
- Multi-user: Session-safe, works with multiple workers

---

## üéØ BONUS: CHUNKING STRATEGY DECISION

You need to decide: **Should instruction files be chunked or kept whole?**

**Current**: Whole file per chunk (despite comments saying otherwise)

**Option A: Keep whole files** (Recommended)
- ‚úÖ Retrieval always returns complete troubleshooting flow
- ‚úÖ No context loss
- ‚ùå Less precise matching (big chunks)
- ‚ùå Fewer vectors in index

**Option B: Intelligent chunking by section**
```python
def chunk_instruction_file(text: str) -> list:
    """Split by section headers but keep step-by-step blocks intact."""
    sections = []
    current_section = []
    
    for line in text.split('\n'):
        # Start new section on major headers
        if line.strip() in ["PROBLEM:", "STEP-BY-STEP RESOLUTION:", "EXPECTED RESULT:"]:
            if current_section:
                sections.append('\n'.join(current_section))
            current_section = [line]
        else:
            current_section.append(line)
    
    if current_section:
        sections.append('\n'.join(current_section))
    
    return sections
```

**My recommendation**: Keep whole files as one chunk for now. Your instruction files are short enough (200-400 words) that they fit comfortably in context windows.

---

## üîç CODE REVIEW SUMMARY

**What's working well**:
- ‚úÖ Clean separation of concerns (main, retriever, llm, ingest)
- ‚úÖ LLM-first approach (always responds)
- ‚úÖ State machine for clarification flow
- ‚úÖ Platform detection logic
- ‚úÖ Structured data files with article links

**What needs immediate attention**:
- üö® Missing FAQ retrieval path
- üö® Global shared state (multi-user unsafe)
- üö® Platform filtering performance
- ‚ö†Ô∏è Course code extraction in follow-ups
- ‚ö†Ô∏è History pollution
- ‚ö†Ô∏è Follow-up context loss

**Architecture recommendations**:
- Session-scoped storage with expiration
- Pre-filtered FAISS indices per platform
- Contextual query enhancement for follow-ups
- Better LLM context injection method
- Consider dual retrieval for better routing

---

Let me know which solution you'd like me to implement first. I can provide complete code for any of the fixes above.